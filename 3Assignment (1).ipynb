{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2COxdlKO_Tx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Other libraries for preprocessing, evaluation, etc. (import as needed)\n",
        "data = pd.read_csv(r\"forestdata.csv\")\n",
        "#col_names = (\"collector.id\",\"c.score\",\"l.score\",\"rain\",\"tree.age\",\"surface.litter\",\"wind.intensity\",\"humidity\",\"tree.density\",\"month\",\"time.of.day\", \"fire\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "\n",
        "# Create an empty DataFrame to store the rows with missing values\n",
        "missing_data_df = pd.DataFrame()\n",
        "\n",
        "# Iterate over each column of the DataFrame\n",
        "for column in data.columns:\n",
        "    # Find rows with missing values in the current column\n",
        "    missing_rows = data[data[column].isnull()]\n",
        "\n",
        "    # If there are missing values in the current column\n",
        "    if not missing_rows.empty:\n",
        "        # Add a new column to indicate the name of the column\n",
        "        missing_rows['Column'] = column\n",
        "\n",
        "        # Concatenate the missing rows to the DataFrame\n",
        "        missing_data_df = pd.concat([missing_data_df, missing_rows])\n",
        "\n",
        "# Reset the index of the resulting DataFrame\n",
        "missing = missing_data_df.reset_index(drop=True)\n",
        "\n",
        "# Display the DataFrame containing rows with missing values for each column\n",
        "print(missing)\n"
      ],
      "metadata": {
        "id": "KpGywBlzuwcH",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target column name and feature column names\n",
        "target_names = \"fire\"\n",
        "feature_names = data.columns[data.columns != target_names]\n",
        "\n",
        "# Extract features and target variable\n",
        "X = data[feature_names]\n",
        "y = data[target_names]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train.drop(columns=['collector.id'], inplace=True)\n",
        "X_test.drop(columns=['collector.id'], inplace=True)"
      ],
      "metadata": {
        "id": "-UPogUExPK-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "metadata": {
        "id": "kT4qv1Kq5UD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "X_train[\"time.of.day\"] = X_train[\"time.of.day\"].replace(\"morni7ng\", \"morning\")\n",
        "\n",
        "sns.countplot(x = \"time.of.day\", data = X_train)\n",
        "plt.title  = (\"Count-Time of day\")\n",
        "plt.xlabel = (\"time.of.day\")\n",
        "plt.ylabel = (\"count\")\n",
        "plt.show\n",
        "\n",
        "X_test[\"time.of.day\"] = X_test[\"time.of.day\"].replace(\"morni7ng\", \"morning\")\n",
        "sns.countplot(x = \"time.of.day\", data = X_test)\n",
        "plt.title = (\"count-time of day\")\n",
        "plt.xlabel = (\"time.of.day\")\n",
        "plt.ylabel = (\"count\")\n",
        "plt.show"
      ],
      "metadata": {
        "id": "-yJHnfYRPQxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "X_train['tree.age'].plot(kind='hist', bins=20, title='tree.age')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "metadata": {
        "id": "ZlMsRTNzgCrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_nasty_rows(feature_names, target_names, X_data, y_data):\n",
        "    \"\"\" manually deciding which feature names to include is a bit strange..\n",
        "    remember to include the index variable or to change which is the index variable.\n",
        "    \"\"\"\n",
        "    result_features = X_data.copy()  # Extract features based on feature_names\n",
        "    result_target = y_data.copy()  # Copy the target data\n",
        "\n",
        "    # Define conditions for dropping rows based on multiple columns\n",
        "    bad_rows = (result_features[\"c.score\"] > 1750) | (result_features[\"tree.age\"] > 500)\n",
        "    # Modify the conditions as needed based on the actual column names in your data\n",
        "\n",
        "    # Filter rows based on the combined conditions\n",
        "    result_features = result_features[~bad_rows]\n",
        "    result_target = result_target[~bad_rows]\n",
        "\n",
        "    return result_features, result_target\n",
        "\n",
        "X_train, y_train = drop_nasty_rows(feature_names, target_names=\"target\", X_data=X_train, y_data=y_train)\n",
        "\n",
        "# Reset indexes of the cleaned data\n",
        "X_train_clean = X_train.reset_index(drop=True)\n",
        "y_train_clean = y_train.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "msSwDJYkFLxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "X_train_clean['tree.age'].plot(kind='hist', bins=20, title='tree.age')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)\n"
      ],
      "metadata": {
        "id": "yi-g1C29e96a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "numeric_columns_train = X_train_clean.select_dtypes(include=['number']).columns\n",
        "categorical_columns_train = X_train_clean.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "# Impute missing values for numeric columns in training data\n",
        "numeric_imputer_train = SimpleImputer(strategy='median')\n",
        "X_train_clean[numeric_columns_train] = numeric_imputer_train.fit_transform(X_train_clean[numeric_columns_train])\n",
        "\n",
        "# Impute missing values for categorical columns in training data\n",
        "categorical_imputer_train = SimpleImputer(strategy='most_frequent')\n",
        "X_train_clean[categorical_columns_train] = categorical_imputer_train.fit_transform(X_train_clean[categorical_columns_train])\n",
        "\n",
        "# Identify numeric and categorical columns in test data\n",
        "numeric_columns_test = X_test.select_dtypes(include=['number']).columns\n",
        "categorical_columns_test = X_test.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "# Impute missing values for numeric columns in test data\n",
        "numeric_imputer_test = SimpleImputer(strategy='median')\n",
        "X_test[numeric_columns_test] = numeric_imputer_test.fit_transform(X_test[numeric_columns_test])\n",
        "\n",
        "# Impute missing values for categorical columns in test data\n",
        "categorical_imputer_test = SimpleImputer(strategy='most_frequent')\n",
        "X_test[categorical_columns_test] = categorical_imputer_test.fit_transform(X_test[categorical_columns_test])\n",
        "\n",
        "# Check for missing values in the imputed training set\n",
        "print(\"Missing values in X_train after imputation:\")\n",
        "print(X_train_clean.isnull().sum())\n",
        "\n",
        "# Check for missing values in the imputed test set\n",
        "print(\"\\nMissing values in X_test after imputation:\")\n",
        "print(X_test.isnull().sum())\n",
        "\n",
        "\n",
        "# Now, X_train and X_test contain imputed values without leakage from the test set"
      ],
      "metadata": {
        "id": "oM-b9nXFkLrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***NORMALIZATION AND ENCODING***"
      ],
      "metadata": {
        "id": "Gq1pGVWtliCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Define the data preparation for the columns\n",
        "#\n",
        "# Categorical columns will have a one hot encoder applied\n",
        "#   (note - we also set this to drop the first \"dummy\" column, similar to what we did last week; we have also\n",
        "#    told it how to handle unknown values, those that don't appear in the training data but do appear in validation\n",
        "#    or testing. 'infrequent_if_exist' assigns any unknown values to a separate category)\n",
        "#\n",
        "# Numerical columns will be normalized\n",
        "#\n",
        "# Each processing step also includes the list of columns that it will be applied to\n",
        "#numerical_X_train = X_train_clean.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "#categorical_X_train = X_train_clean.select_dtypes(include=[\"object\", \"bool\"]).columns\n",
        "numerical_X_train = [\"c.score\", \"l.score\", \"rain\", \"tree.age\", \"surface.litter\", \"wind.intensity\", \"humidity\", \"tree.density\", \"month\"]\n",
        "categorical_X_train = [\"time.of.day\"]\n",
        "\n",
        "print(numerical_X_train)\n",
        "print(categorical_X_train)\n",
        "\n",
        "t = [('cat', OneHotEncoder(drop='first', handle_unknown='infrequent_if_exist'), categorical_X_train), ('num', MinMaxScaler(), numerical_X_train)]\n",
        "col_transform = ColumnTransformer(transformers=t)\n",
        "\n",
        "# Fit() works out which columns to make and stores them in \"enc\"\n",
        "col_transform.fit(X_train_clean)\n",
        "\n",
        "# Transform() then makes those columns in the data set we provide\n",
        "X_train_encod = col_transform.transform(X_train_clean)\n",
        "X_test_encod = col_transform.transform(X_test)  # Transform X_test as well\n",
        "\n",
        "# X_train_encoded and X_test_encoded will now be numpy arrays, which can be used for training and testing\n",
        "# The columns in these are the transformed data\n",
        "train_view = pd.DataFrame(X_train_encod)\n",
        "test_view = pd.DataFrame(X_test_encod)\n",
        "\n",
        "train_view.head(10)\n",
        "test_view.head(10)\n"
      ],
      "metadata": {
        "id": "6qzQeYSiyOKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9WMWTTPmsCSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assuming you already have split data (X_train, y_train)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "#Apply SMOTE only to the training data\n",
        "X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train_encod, y_train)"
      ],
      "metadata": {
        "id": "DEqDvoHGlD1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define models with default hyperparameters\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=37)\n",
        "logistic_regression_model = LogisticRegression(random_state=37)\n",
        "neural_network_model = MLPClassifier(random_state=37)\n",
        "\n",
        "# Define hyperparameter grids\n",
        "decision_tree_params = {\n",
        "    'max_depth': [None, 5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "logistic_regression_params = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "neural_network_params = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "    'activation': ['logistic', 'relu'],\n",
        "    'alpha': [0.0001, 0.001, 0.01]\n",
        "}\n",
        "\n",
        "# Define evaluation metrics\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'roc_auc': make_scorer(roc_auc_score)\n",
        "}\n",
        "\n",
        "# GridSearchCV for each model with recall scoring\n",
        "decision_tree_grid = GridSearchCV(decision_tree_model, decision_tree_params, scoring=scoring, refit='recall', cv=5)\n",
        "logistic_regression_grid = GridSearchCV(logistic_regression_model, logistic_regression_params, scoring=scoring, refit='recall', cv=5)\n",
        "neural_network_grid = GridSearchCV(neural_network_model, neural_network_params, scoring=scoring, refit='recall', cv=5)\n",
        "\n",
        "# Fit models\n",
        "decision_tree_grid.fit(X_train_resampled, y_train_resampled)\n",
        "logistic_regression_grid.fit(X_train_resampled, y_train_resampled)\n",
        "neural_network_grid.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "\n",
        "# Get best models\n",
        "best_decision_tree_model = decision_tree_grid.best_estimator_\n",
        "best_logistic_regression_model = logistic_regression_grid.best_estimator_\n",
        "best_neural_network_model = neural_network_grid.best_estimator_\n"
      ],
      "metadata": {
        "id": "v4kSZOjR3psx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get best hyperparameters from GridSearchCV (Cell 1)\n",
        "best_decision_tree_params = decision_tree_grid.best_params_\n",
        "best_logistic_regression_params = logistic_regression_grid.best_params_\n",
        "best_neural_network_params = neural_network_grid.best_params_\n",
        "\n",
        "# Train the models with best hyperparameters\n",
        "best_decision_tree_model.fit(X_train_resampled, y_train_resampled)\n",
        "best_logistic_regression_model.fit(X_train_resampled, y_train_resampled)\n",
        "best_neural_network_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "decision_tree_predictions = best_decision_tree_model.predict(X_test_encod)\n",
        "logistic_regression_predictions = best_logistic_regression_model.predict(X_test_encod)\n",
        "neural_network_predictions = best_neural_network_model.predict(X_test_encod)\n",
        "\n",
        "# Print training completion message (optional)\n",
        "print(\"Final Training Completed!\")\n"
      ],
      "metadata": {
        "id": "ZLszKhr-0nzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " decision_tree_grid.best_params_\n"
      ],
      "metadata": {
        "id": "-29cdCiFb3ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " logistic_regression_grid.best_params_\n",
        "\n"
      ],
      "metadata": {
        "id": "S2An5g_mdTAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " neural_network_grid.best_params_\n"
      ],
      "metadata": {
        "id": "I6-J4cEVdUJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Evaluate models\n",
        "decision_tree_accuracy = accuracy_score(y_test, decision_tree_predictions)\n",
        "logistic_regression_accuracy = accuracy_score(y_test, logistic_regression_predictions)\n",
        "neural_network_accuracy = accuracy_score(y_test, neural_network_predictions)\n",
        "\n",
        "decision_tree_precision = precision_score(y_test, decision_tree_predictions)\n",
        "logistic_regression_precision = precision_score(y_test, logistic_regression_predictions)\n",
        "neural_network_precision = precision_score(y_test, neural_network_predictions)\n",
        "\n",
        "decision_tree_recall = recall_score(y_test, decision_tree_predictions)\n",
        "logistic_regression_recall = recall_score(y_test, logistic_regression_predictions)\n",
        "neural_network_recall = recall_score(y_test, neural_network_predictions)\n",
        "\n",
        "decision_tree_f1 = f1_score(y_test, decision_tree_predictions)\n",
        "logistic_regression_f1 = f1_score(y_test, logistic_regression_predictions)\n",
        "neural_network_f1 = f1_score(y_test, neural_network_predictions)\n",
        "\n",
        "decision_tree_auc = roc_auc_score(y_test, decision_tree_predictions)\n",
        "logistic_regression_auc = roc_auc_score(y_test, logistic_regression_predictions)\n",
        "neural_network_auc = roc_auc_score(y_test, neural_network_predictions)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Decision Tree Metrics:\")\n",
        "print(\"Accuracy:\", decision_tree_accuracy)\n",
        "print(\"Precision:\", decision_tree_precision)\n",
        "print(\"Recall:\", decision_tree_recall)\n",
        "print(\"F1 Score:\", decision_tree_f1)\n",
        "print(\"AUC:\", decision_tree_auc)\n",
        "\n",
        "print(\"\\nLogistic Regression Metrics:\")\n",
        "print(\"Accuracy:\", logistic_regression_accuracy)\n",
        "print(\"Precision:\", logistic_regression_precision)\n",
        "print(\"Recall:\", logistic_regression_recall)\n",
        "print(\"F1 Score:\", logistic_regression_f1)\n",
        "print(\"AUC:\", logistic_regression_auc)\n",
        "\n",
        "print(\"\\nNeural Network Metrics:\")\n",
        "print(\"Accuracy:\", neural_network_accuracy)\n",
        "print(\"Precision:\", neural_network_precision)\n",
        "print(\"Recall:\", neural_network_recall)\n",
        "print(\"F1 Score:\", neural_network_f1)\n",
        "print(\"AUC:\", neural_network_auc)\n"
      ],
      "metadata": {
        "id": "lATeO0sAzl8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "y_pred = best_neural_network_model.predict(X_test_encod)\n",
        "print(\"r2:\", metrics.r2_score(y_test, y_pred))\n",
        "print(\"MSE:\", metrics.mean_squared_error(y_test, y_pred))\n",
        "\n",
        "\n",
        "# how many under 1 min, 3 mins?\n",
        "#predDifferences = abs(y_pred - y_test)\n",
        "#np.where(predDifferences < 3)\n",
        "predDifferences = abs(np.subtract(y_pred, y_test).ravel())\n",
        "print(\"Fraction < 1min:\", len(np.where(predDifferences < 1)[0]) / len(y_test))\n",
        "print(\"Fraction < 3min:\", len(np.where(predDifferences < 3)[0]) / len(y_test))\n",
        "\n",
        "# distributions\n",
        "hist, bins = np.histogram(predDifferences, bins=range(0,15,1))\n",
        "width = 0.7 * (bins[1] - bins[0])\n",
        "center = (bins[:-1] + bins[1:]) / 2\n",
        "plt.bar(center, hist, align='center', width=width)"
      ],
      "metadata": {
        "id": "sWWLYhRiloh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assuming you have true labels (y_test) and predicted labels (e.g., decision_tree_predictions)\n",
        "confusion_matrix_dt = confusion_matrix(y_test, decision_tree_predictions)\n",
        "confusion_matrix_lr = confusion_matrix(y_test, logistic_regression_predictions)\n",
        "confusion_matrix_nn = confusion_matrix(y_test, neural_network_predictions)\n",
        "\n",
        "# Print the confusion matrices for each model\n",
        "print(\"Decision Tree Confusion Matrix:\\n\", confusion_matrix_dt)\n",
        "print(\"Logistic Regression Confusion Matrix:\\n\", confusion_matrix_lr)\n",
        "print(\"Neural Network Confusion Matrix:\\n\", confusion_matrix_nn)\n"
      ],
      "metadata": {
        "id": "Y3wFV3sJgbuH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
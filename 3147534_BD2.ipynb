{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du7mxRxZ6WV-"
      },
      "source": [
        "# University of Stirling\n",
        "\n",
        "# ITNPBD2 Representing and Manipulating Data\n",
        "\n",
        "# Assignment Autumn 2023\n",
        "\n",
        "# A Consultancy Job for JC Penney\n",
        "\n",
        "This notebook forms the assignment instructions and submission document of the assignment for ITNPBD2. Read the instructions carefully and enter code into the cells as indicated.\n",
        "\n",
        "You will need these five files, which were in the Zip file you downloaded from the course webpage:\n",
        "\n",
        "- jcpenney_reviewers.json\n",
        "- jcpenney_products.json\n",
        "- products.csv\n",
        "- reviews.csv\n",
        "- users.csv\n",
        "\n",
        "The data in these files describes products that have been sold by the American retail giant, JC Penney, and reviews by customers who bought them. Note that the product data is real, but the customer data is synthetic.\n",
        "\n",
        "Your job is to process the data, as requested in the instructions in the markdown cells in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDOPPENO6WWA"
      },
      "source": [
        "# Completing the Assignment\n",
        "\n",
        "Rename this file to be xxxxxx_BD2 where xxxxxx is your student number, then type your code and narrative description into the boxes provided. Add as many code and markdown cells as you need. The cells should contain:\n",
        "\n",
        "- **Text narrative describing what you did with the data**\n",
        "- **The code that performs the task you have described**\n",
        "- **Comments that explain your code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT4alUo96WWA"
      },
      "source": [
        "# Marking Scheme\n",
        "The assessment will be marked against the university Common Marking Scheme (CMS)\n",
        "\n",
        "Here is a summary of what you need to achieve to gain a grade in the major grade bands:\n",
        "\n",
        "|Grade|Requirement|\n",
        "|:---|:---|\n",
        "| Fail | You will fail if your code does not run or does not achieve even the basics of the task. You may also fail if you submit code without either comments or a text explanation of what the code does.|\n",
        "| Pass | To pass, you must submit sufficient working code to show that you have mastered the basics of the task, even if not everything works completely. You must include some justifications for your choice of methods, but without mentioning alternatives. |\n",
        "| Merit | For a merit, your code must be mostly correct, with only small problems or parts missing, and your comments must be useful rather than simply re-stating the code in English. Most choices for methods and structures should be explained and alternatives mentioned. |\n",
        "| Distinction | For a distinction, your code must be working, correct, and well commented and shows an appreciation of style, efficiency and reliability. All choices for methods and structures are concisely justified and alternatives are given well thought considerations. For a distinction, your work should be good enough to present to executives at the company.|\n",
        "\n",
        "The full details of the CMS can be found here\n",
        "\n",
        "https://www.stir.ac.uk/about/professional-services/student-academic-and-corporate-services/academic-registry/academic-policy-and-practice/quality-handbook/assessment-policy-and-procedure/appendix-2-postgraduate-common-marking-scheme/\n",
        "\n",
        "Note that this means there are not certain numbers of marks allocated to each stage of the assignment. Your grade will reflect how well your solutions and comments demonstrate that you have achieved the learning outcomes of the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12o2GZMc6WWM"
      },
      "source": [
        "## Submission\n",
        "When you are ready to submit, **print** your notebook as PDF (go to File -> Print Preview) in the Jupyter menu. Make sure you have run all the cells and that their output is displayed. Any lines of code or comments that are not visible in the pdf should be broken across several lines. You can then submit the file online.\n",
        "\n",
        "Late penalties will apply at a rate of three marks per day, up to a maximum of 7 days. After 7 days you will be given a mark of 0. Extensions will be considered under acceptable circumstances outside your control.\n",
        "\n",
        "## Academic Integrity\n",
        "\n",
        "This is an individual assignment, and so all submitted work must be fully your own work.\n",
        "\n",
        "The University of Stirling is committed to protecting the quality and standards of its awards. Consequently, the University seeks to promote and nurture academic integrity, support staff academic integrity, and support students to understand and develop good academic skills that facilitate academic integrity.\n",
        "\n",
        "In addition, the University deals decisively with all forms of Academic Misconduct.\n",
        "\n",
        "Where a student does not act with academic integrity, their work or behaviour may demonstrate Poor Academic Practice or it may represent Academic Misconduct.\n",
        "\n",
        "### Poor Academic Practice\n",
        "\n",
        "Poor Academic Practice is defined as: \"The submission of any type of assessment with a lack of referencing or inadequate referencing which does not effectively acknowledge the origin of words, ideas, images, tables, diagrams, maps, code, sound and any other sources used in the assessment.\"\n",
        "\n",
        "### Academic Misconduct\n",
        "\n",
        "Academic Misconduct is defined as: \"any act or attempted act that does not demonstrate academic integrity and that may result in creating an unfair academic advantage for you or another person, or an academic disadvantage for any other member or member of the academic community.\"\n",
        "\n",
        "Plagiarism is presenting somebody elseâ€™s work as your own **and includes the use of artificial intelligence tools such as GPT or CoPilot**. Plagiarism is a form of academic misconduct and is taken very seriously by the University. Students found to have plagiarised work can have marks deducted and, in serious cases, even be expelled from the University. Do not submit any work that is not entirely your own. Do not collaborate with or get help from anybody else with this assignment.\n",
        "\n",
        "The University of Stirling's full policy on Academic Integrity can be found at:\n",
        "\n",
        "https://www.stir.ac.uk/about/professional-services/student-academic-and-corporate-services/academic-registry/academic-policy-and-practice/quality-handbook/academic-integrity-policy-and-academic-misconduct-procedure/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqCq1ka56WWN"
      },
      "source": [
        "## The Assignment\n",
        "Your task with this assignment is to use the data provided to demonstrate your Python data manipulation skills.\n",
        "\n",
        "There are three `.csv` files and two `.json` files so you can process different types of data. The files also contain unstructured data in the form of natural language in English and links to images that you can access from the JC Penney website (use the field called `product_image_urls`).\n",
        "\n",
        "Start with easy tasks to show you can read in a file, create some variables and data structures, and manipulate their contents. Then move onto something more interesting.\n",
        "\n",
        "Look at the data that we provided with this assessment and think of something interesting to do with it using whatever libraries you like. Describe what you decide to do with the data and why it might be interesting or useful to the company to do it.\n",
        "\n",
        "You can add additional data if you need to - either download it or access it using `requests`. Produce working code to implement your ideas in as many cells as you need below. There is no single right answer, the aim is to simply show you are competent in using python for data analysis. Exactly how you do that is up to you.\n",
        "\n",
        "For a distinction class grade, this must show originality, creative thinking, and insights beyond what you've been taught directly on the module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z663PaeV6WWO"
      },
      "source": [
        "## Structure\n",
        "You may structure the project how you wish, but here is a suggested guideline to help you organise your work:\n",
        "\n",
        " 1. Data Exploration - Explore the data and show you understand its structure and relations\n",
        " 2. Data Validation - Check the quality of the data. Is it complete? Are there obvious errors?\n",
        " 3. Data Visualisation - Gain an overall understanding of the data with visualisations\n",
        " 4. Data Analysis = Set some questions and use the data to answer them\n",
        " 5. Data Augmentation - Add new data from another source to bring new insights to the data you already have"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS1VtdXd6WWO"
      },
      "source": [
        "# Remember to make sure you are working completely on your own.\n",
        "# Don't work in a group or with a friend\n",
        "You may NOT use any automated code generation or analytics tools for this assignment, so do not use tools like GPT. You can look up the syntax for the functions you use, but you must write the code yourself and the comments must provide an insightful analysis of the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZrVrYuH6WWO"
      },
      "source": [
        "# 1.0 Installation and importation of the necessary libraries\n",
        "In order to run an effective analysis, we need to install and import several libaries.\n",
        "\n",
        "The list of libraries needed was updated at several stages of the analysis when the need arose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWNRADLP6WWP"
      },
      "outputs": [],
      "source": [
        "#This was installed in order to import the us library\n",
        "#!pip install us\n",
        "#This was installed in order to import the wordcloud library\n",
        "#!pip install wordcloud\n",
        "\n",
        "#These libararies will be helpful in the various stages of the analysis. It will help with data cleaning, transformation etc.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Visualization libraries: These libraries are needed for various forms of visualization.\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('punkt')\n",
        "from scipy.stats import pearsonr\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "#Needed in order to get the abbreviations of the names of the states in the USA\n",
        "import us\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWs-WOQT6WWQ"
      },
      "source": [
        "# 2.0 IMPORTATION OF DATA\n",
        "The files provided for the analysis which includes: product.csv, reviews.csv, user.csv , jcpenney_products.json, jcpenney_reviewers.json; were imported using the pandas library. The syntax is basically\n",
        "      New_file_name = pd.read(r\"file path/original file name')\n",
        "      \n",
        "Addtionally, the following excel files:\n",
        "\n",
        "\"JC-PENNY_number_of_stores_by_state_USA.xlsx\" and\n",
        "\"edit_us-per-capita-personal-income-by-state.xlsx\"\n",
        "\n",
        "were imported from\n",
        "\n",
        "https://www.statista.com/statistics/860987/number-of-stores-of-jcpenney-by-state/ and\n",
        "https://www.statista.com/statistics/303555/us-per-capita-personal-income/\n",
        "\n",
        "respectively. These new files wll help add more insight to the data already provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnaJy5p06WWQ"
      },
      "outputs": [],
      "source": [
        "#1.0 This piece of code below ensures that all columns are displayed and none is cut off. it was included it to ensure\n",
        "#I could see all the relevant information especially on the json file which cannot be viewed on excel\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "#The rest of these codes just imports the data and assisgs them to new file names namely: product_c, review_c e.t.c.\n",
        "product_c = pd.read_csv(r\"C:\\Users\\emerald\\1Main assignment\\Python Scripts\\products.csv\")\n",
        "review_c = pd.read_csv(r\"C:\\Users\\emerald\\1Main assignment\\Python Scripts\\reviews.csv\")\n",
        "user_review_c = pd.read_csv(r\"C:\\Users\\emerald\\1Main assignment\\Python Scripts\\users.csv\")\n",
        "product_j = pd.read_json(r\"C:\\Users\\emerald\\1Main assignment\\Python Scripts\\jcpenney_products.json\", lines = True)\n",
        "user_review_j = pd.read_json(r\"C:\\Users\\emerald\\1Main assignment\\Python Scripts\\jcpenney_reviewers.json\", lines = True)\n",
        "\n",
        "number_of_store=pd.read_excel(r\"C:\\Users\\emerald\\1Main assignment\\Python Scripts\\JC-PENNY_number_of_stores_by_state_USA.xlsx\",\n",
        "                                 names = [\"State\",\"Number of stores\"])\n",
        "\n",
        "Per_capita_income=pd.read_excel(r\"C:\\Users\\emerald\\1Main assignment\\Python Scripts\\edit_us-per-capita-personal-income-by-state.xlsx\",\n",
        "                                 names = [\"State\",\"Per Capita Personal Income\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrB__TId6WWR"
      },
      "outputs": [],
      "source": [
        "Per_capita_income.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9PBcmHl6WWS"
      },
      "outputs": [],
      "source": [
        "number_of_store.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkSjpVxp6WWS"
      },
      "source": [
        "# 4. INSPECTING AND VALIDATING THE DATA\n",
        "\n",
        "At this stage, I look at the structures of the data, its similarities, potential for merging, null values, duplicates, etc. The actions taken here set the stage for how the next stages will go.\n",
        "\n",
        "Note: It is very possible to overlook or miss important details; it is always okay to return to the drawing board."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x02UAMoU6WWS"
      },
      "source": [
        "### 4.1 DATA INSPECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYQ4KEEd6WWS"
      },
      "source": [
        "4.11 Inspecting the product csv and JSON files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqOftzrE6WWS"
      },
      "outputs": [],
      "source": [
        "product_j.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-d3EO0t6WWS"
      },
      "outputs": [],
      "source": [
        "product_c.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHtK2nR16WWT"
      },
      "source": [
        "At first glance, the product csv file and the product JSON file had similarities, so I had to compare them further to be sure.On inspection, I discovered that they are almost the same, the difference being that the JSON file had more columns than the CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArtB7pUe6WWT"
      },
      "outputs": [],
      "source": [
        "#Inspecting the first 3 and last 3 rows columns the product csv file\n",
        "product_c.head(3)\n",
        "product_c.tail(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rblReyJg6WWT"
      },
      "outputs": [],
      "source": [
        "#Inspecting the first 3 and last 3 rows columns the product JSON file\n",
        "product_j.head(3)\n",
        "product_j.tail(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrQwu93L6WWT"
      },
      "outputs": [],
      "source": [
        "#Inspecting a section of rows (1860 -1870) product csv file\n",
        "product_c.iloc[1860:1870]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAdpJRCw6WWT"
      },
      "outputs": [],
      "source": [
        "#Inspecting a section of rows (1860 -1870) product JSON file\n",
        "product_j.iloc[1860:1870]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LV01bPi6WWT"
      },
      "outputs": [],
      "source": [
        "#Inspecting the unique number of goods in the product csv file\n",
        "unique_names = product_c['Name'].value_counts()\n",
        "unique_names.head(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNlR0pgc6WWT"
      },
      "outputs": [],
      "source": [
        "#Inspecting the unique number of goods in the product JSON file\n",
        "unique_name_title = product_j['name_title'].value_counts()\n",
        "unique_name_title.head(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2yDw36M6WWU"
      },
      "outputs": [],
      "source": [
        "#Inspecting the summaries of the numerical data\n",
        "product_c.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65NgHsUb6WWU"
      },
      "outputs": [],
      "source": [
        "#Inspecting the summaries of the numerical data\n",
        "product_j.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmEMVQNn6WWU"
      },
      "source": [
        "There are quite a considerable amount of similarites to draw the conclusion that the product JSON file is a detailed form of the product csv file. I will take the product JSON files as the representative of the two, whatever necessary cleaning and transformation will be done on the product JSON file.\n",
        "\n",
        "The only unique column in the product csv is the price column and it has too many issues in forms of  missing values and negative values, that I would rather go with the sale price column as a representative of the both."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "i5zuEoZd6WWU"
      },
      "source": [
        "4.12 Inspecting the user review csv and review JSON files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm4qAfzL6WWU"
      },
      "outputs": [],
      "source": [
        "user_review_c.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZfD4KX86WWU"
      },
      "outputs": [],
      "source": [
        "user_review_j.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62iVsUr16WWU"
      },
      "outputs": [],
      "source": [
        "user_review_c.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iPrR5eP6WWV"
      },
      "outputs": [],
      "source": [
        "user_review_j.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYO9lN146WWV"
      },
      "outputs": [],
      "source": [
        "user_review_c.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNPtx82y6WWV"
      },
      "outputs": [],
      "source": [
        "user_review_j.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d3qvcm06WWV"
      },
      "outputs": [],
      "source": [
        "review_c.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xyIu6T26WWV"
      },
      "source": [
        "There is also ample evidence to suggest that both files are almost the same, difference being that the JSON file has more columns than the csv file. Again, I will take the JSON file as the representative of the two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WCfO40e6WWV"
      },
      "source": [
        "4.13 Inspecting the review csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjDluEzB6WWV"
      },
      "outputs": [],
      "source": [
        "review_c.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHFlwXLh6WWW"
      },
      "outputs": [],
      "source": [
        "review_c.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbnCIy886WWW"
      },
      "outputs": [],
      "source": [
        "review_c.describe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPdp19sV6WWW"
      },
      "source": [
        "### 4.2 DATA VALIDATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfymFN7e6WWW"
      },
      "source": [
        "4.21 Checking for duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s_t12G96WWX"
      },
      "outputs": [],
      "source": [
        "#This code checks if there are duplicates in the files\n",
        "product_c.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOwXiEJY6WWX"
      },
      "outputs": [],
      "source": [
        "user_review_c.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeVldNUb6WWX"
      },
      "outputs": [],
      "source": [
        "review_c.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOhlyAOc6WWX"
      },
      "source": [
        "No duplicates detected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mlB5OaG6WWX"
      },
      "source": [
        "4.22 Checking for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-ZhEr0Y6WWX"
      },
      "outputs": [],
      "source": [
        "#This code checks for missing values in the rows and columns\n",
        "product_c.isnull().any().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0TESUgU6WWX"
      },
      "outputs": [],
      "source": [
        "product_c.isnull().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56UL0RUU6WWY"
      },
      "outputs": [],
      "source": [
        "user_review_c.isnull().any().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6rXW-ra6WWY"
      },
      "outputs": [],
      "source": [
        "user_review_j.isnull().any().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaRtfCVv6WWY"
      },
      "outputs": [],
      "source": [
        "review_c.isnull().any().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr2IskZ56WWY"
      },
      "source": [
        "The results above show that the product csv and product json files have null values. I will proceed to fix it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn7MRItT6WWY"
      },
      "source": [
        "# 5.0 DATA CLEANING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkK2v8Gq6WWY"
      },
      "source": [
        "Cleaning the product.json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KL2N53A6WWY"
      },
      "outputs": [],
      "source": [
        "product_c.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESbTm56D6WWZ"
      },
      "outputs": [],
      "source": [
        "#Checking percentage of null values in the product file\n",
        "\n",
        "Percentage_null_on_price = (product_c[\"Price\"].isnull().sum()/len(product_c))*100\n",
        "print(f\" Percentage of price value that is missing in the product_csv file is {Percentage_null_on_price: .2f}% \")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "d6xw6qjh6WWa"
      },
      "source": [
        "There are too many list prices are missing on products_csv file, which makes it unsuitable to run an unbiased analysis. I will have to go ahead and check the sales price on the json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO8juRGd6WWa"
      },
      "outputs": [],
      "source": [
        "product_j.isnull().any().any()\n",
        "if product_j.isnull().any().any():\n",
        "    print(\"omitted values present\")\n",
        "else:\n",
        "    print(\"omitted values absent\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "msGUe3hZ6WWa"
      },
      "source": [
        "I was not able to call out the missing values in the json file,even though there are actually missing values.I tried something esle.\n",
        "\n",
        "I tried to identify where the null values are on the product_csv file  and then compared it with the product_JSON file, like I suspected, there were missing values and range values on the sale price column which were inconsistent with the rest of the entires. I needed to fix both as they can pose as a hindrance to progress during the latter stages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dolWk1Yo6WWa"
      },
      "outputs": [],
      "source": [
        "missing_product_c = product_c[product_c.isnull().any(axis=1)]\n",
        "missing_product_c\n",
        "product_c.iloc[1788:1799]\n",
        "product_c.iloc[1860:1870]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c-lzxGq6WWa"
      },
      "outputs": [],
      "source": [
        "missing_product_j = product_c[product_c.isnull().any(axis=1)]\n",
        "missing_product_j\n",
        "product_j.iloc[1788:1799]\n",
        "product_j.iloc[1860:1870]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DELbEd2k6WWa"
      },
      "source": [
        "In order to fix the issue with the product json, I had to do the following:\n",
        "\n",
        "1. Convert the range prices to mean of the two values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI9Vr0Ti6WWa"
      },
      "outputs": [],
      "source": [
        "#I created a function that checks for '-' sign, removes it and calculates the average of the prices.\n",
        "\n",
        "def standardize_price(price):\n",
        "    if isinstance(price, str) and '-' in price:\n",
        "        start, end = map(float, price.split('-'))\n",
        "        return (start + end) / 2\n",
        "    else:\n",
        "        return (price)\n",
        "\n",
        "#I applied the function to the sale price column of the the product_JSON file\n",
        "product_j['sale_price'] = product_j['sale_price'].apply(standardize_price)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJBKOCUB6WWb"
      },
      "source": [
        "2. Converted the object type data to numbers and converts the missing values to 'NA'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sIsjfwj6WWb"
      },
      "outputs": [],
      "source": [
        "#2. Convert the string values to floats and force the missing values to assume the value \"NA\"\n",
        "product_j['sale_price'] = pd.to_numeric(product_j['sale_price'], errors = 'coerce')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s272xd376WWb"
      },
      "source": [
        "3. Checked how many null values I had, to determine if it is safe to delete them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNY9n38H6WWb"
      },
      "outputs": [],
      "source": [
        "#I had to check how many missing values I have left to determine if it is safe to delete them\n",
        "\n",
        "product_j[\"sale_price\"].isnull().value_counts()\n",
        "# 18 rows in the sales_price column are omitted. It is safe to remove them as the percentage is quite small to create\n",
        "# a bias\n",
        "\n",
        "Percentage_null_on_sale_price = (product_j[\"sale_price\"].isnull().sum()/len(product_j))*100\n",
        "print(f\" Percentage of sale price value that is missing in the product_json file is {Percentage_null_on_sale_price: .2f}% \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAgHoNMB6WWb"
      },
      "source": [
        "18 rows which represents 0.23% of the total number of rows is quite insignificant and can be safely removed without running the risk of introducting some sort of bias.\n",
        "\n",
        "So deleting it,  I did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7QEgn5x6WWb"
      },
      "outputs": [],
      "source": [
        "#The deleting is achieve by using the \"file_name.dropna()\" syntax\n",
        "Cleaned_product_j = product_j.dropna()\n",
        "\n",
        "#Then the code: file_name['cleaned column'].isnull.value_counts(), confirms that the nulls are gone.\n",
        "Cleaned_product_j[\"sale_price\"].isnull().value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ifd8Toxx6WWc"
      },
      "source": [
        "Note: I actually altered the data two externally sourced data, that is, \"JC-PENNY_number_of_stores_by_state_USA.xlsx\", \"Python Scripts\\edit_us-per-capita-personal-income-by-state.xlsx\". Since the rows were few, I edited it a bit using Excel.\n",
        "\n",
        "The review csv file given has all the states in the US including DC and 5 terretories namely: Guam, Nothern Mariana Island, American Samoa, District of Columbia, Minor Outlying Islands, US Virgins Island  . These were missing in the externally sourced files, so I manually included them and sorted all the files in alphabetical order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU8iWhMm6WWc"
      },
      "source": [
        "# 6.0 DATA TRANSFORMATION\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOXbnKQL6WWc"
      },
      "source": [
        "### 6.1 Transforming product_j file which is now named cleaned_product_j\n",
        "\n",
        "The following columns will be deleted based on, unnecessary and too many missing values:\n",
        "sku', 'description', 'list_price','product_url', 'product_image_urls', 'brand','Reviews', 'Bought With'\n",
        "\n",
        "\"sku\", \"description\" and \"list_price\" columns will be deleted because of the siginificat number of missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab_CrIU16WWc"
      },
      "outputs": [],
      "source": [
        "#RestrucRestructuring the product JSON file by picking columns relevant to the analysis\n",
        "transformed_product_j = Cleaned_product_j[['uniq_id', 'name_title','brand','sale_price',\n",
        "                                           'average_product_rating', 'total_number_reviews']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "gVecAnc56WWc"
      },
      "outputs": [],
      "source": [
        "#Created two extra columns,\n",
        "#Total_sale_price; by sum the sale price unique to each product\n",
        "transformed_product_j[\"Total_sale_price\"] = product_j.groupby(\"name_title\")[\"sale_price\"].transform('sum')\n",
        "#Number of each unqiue product sold\n",
        "transformed_product_j[\"Total_goods\"] = product_j.groupby(\"name_title\")[\"name_title\"].transform('count')\n",
        "\n",
        "transformed_product_j"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "-4U5JleN6WWc"
      },
      "source": [
        "I had to calculate the median sale price instead of the mean because, in price data, where a small percentage of individuals might have relatively more expensive goods, the median is often used to describe the central tendency. This prevents the mean from being heavily influenced by a small number of very high values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJVd2rZ96WWc"
      },
      "outputs": [],
      "source": [
        "\n",
        "above_median_df = transformed_product_j[transformed_product_j['sale_price'] >\n",
        "                                        transformed_product_j['sale_price'].median()]\n",
        "num_above_median = len(above_median_df)\n",
        "\n",
        "print(num_above_median)\n",
        "\n",
        "below_median_df = transformed_product_j[transformed_product_j['sale_price'] <\n",
        "                                        transformed_product_j['sale_price'].median()]\n",
        "num_below_median = len(above_median_df)\n",
        "\n",
        "print(num_below_median)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4sycygY6WWd"
      },
      "outputs": [],
      "source": [
        "\n",
        "sale_prices = transformed_product_j['sale_price'].to_numpy()\n",
        "\n",
        "# Calculate the mean average sale price\n",
        "median_sale_price = np.median(sale_prices)\n",
        "\n",
        "print(f\" The average sale price is {median_sale_price}\")"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "tRSLO9in6WWd"
      },
      "source": [
        "### 6.2 Transforming review_c file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY-eazS96WWd"
      },
      "source": [
        "### 6.3 Transforming User_review_j file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNe2U6Ga6WWd"
      },
      "source": [
        "### 6.31 Adding two columns:\n",
        "    \n",
        "State_Frequency which will state how many reviews were recieved from each state\n",
        "\n",
        "Age_Category which will tell the age distributuion of those who partook in the review\n",
        "\n",
        "The data on these new columns will help in the later stages of the analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-vbTGDw6WWd"
      },
      "outputs": [],
      "source": [
        "#Creating a new column named \"State_frequency\"\n",
        "user_review_j[\"State_frequency\"] = user_review_j.groupby(\"State\")[\"State\"].transform('count')\n",
        "\n",
        "#Using the datetime keyword to harmonise the date format and fix a current date\n",
        "user_review_j['DOB'] = pd.to_datetime(user_review_j['DOB'], format='%d.%m.%Y')\n",
        "current_date = pd.to_datetime(\"2019 -07-22\")\n",
        "\n",
        "#Using a combo of various keywords to categorize the ages\n",
        "user_review_j[\"Age\"] =(current_date - user_review_j[\"DOB\"]).astype(\"<m8[Y]\")\n",
        "bins = [0, 12, 18, 25, 60, float('inf')]\n",
        "labels = ['Child', 'Adolescence', 'Young Adult', 'Adult', 'Aged']\n",
        "user_review_j['Age_category'] = pd.cut(user_review_j[\"Age\"], bins = bins, labels = labels, right = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW8EK5K06WWd"
      },
      "source": [
        "### 6.4 Creating a new data frame\n",
        "\n",
        "To gain more insight from the data, there is need to argument the existing ones that with relevant information.\n",
        "\n",
        "Two columns from the US per capita personal income and number of PC Jenny stores in the US will attached to the some columns from the number of reviews gotten from the state (and some districts) in US\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiPY6Gje6WWd"
      },
      "outputs": [],
      "source": [
        "#Calculate the frequencies of the state\n",
        "State_value_counts = user_review_j['State'].value_counts()\n",
        "\n",
        "# Sort the unique values of 'Name' alphabetically\n",
        "sorted_names = sorted(user_review_j['State'].unique())\n",
        "\n",
        "# Create a new DataFrame with sorted names and corresponding  frequencies\n",
        "edited_user_review=pd.DataFrame({'State': sorted_names, 'Review per state': State_value_counts[sorted_names].values})\n",
        "\n",
        "#Attached two new colums 'Number of stores' and 'Per Capita Personal Income' form the two imported files.\n",
        "edited_user_review['Number of stores'] = number_of_store['Number of stores']\n",
        "edited_user_review['Per_capita_income'] = Per_capita_income['Per Capita Personal Income']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjNpiBg56WWe"
      },
      "outputs": [],
      "source": [
        "#Calculating average income per month of individuals in the various states in the Us\n",
        "average_income_per_month = edited_user_review['Per_capita_income'].mean()/12\n",
        "average_income_per_month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38wTux2i6WWe"
      },
      "outputs": [],
      "source": [
        "#The files going to be used for the visualization are :\n",
        "transformed_user_review_j = user_review_j\n",
        "transformed_review_c = review_c\n",
        "transformed_product_j\n",
        "edited_user_review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9ZcpVmj6WWe"
      },
      "source": [
        "\n",
        "\n",
        "# 7.0 DATA VISUALIZATION\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kcal40h6WWe"
      },
      "source": [
        "### 7.1 Price distribution\n",
        "\n",
        "A heat map was picked to visualize price distribution due to its suitability in sales and marketing analysis and the volume of unique data given.\n",
        "The products had a wide range of unique names, so bar charts and some other forms of visualization might not be suitable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PCUPIxm6WWe"
      },
      "outputs": [],
      "source": [
        "#Plotiing the heat map\n",
        "heatmap_data = transformed_product_j.pivot_table(index='name_title', values='sale_price', aggfunc='sum')\n",
        "plt.figure(figsize=(12, 8))\n",
        "sn.heatmap(heatmap_data, cmap='inferno')\n",
        "plt.title('Heatmap of Sets of Goods vs Total Prices')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "qHpo-_ti6WWf"
      },
      "source": [
        "The almost perfect uniform coloration implies that most prices fall within a price range (<2000) with just a few outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcwXr9UD6WWf"
      },
      "outputs": [],
      "source": [
        "#Plotting the bar chart\n",
        "plt.bar(['Above Median', 'Below Median'], [num_above_median, num_below_median], color=['green', 'red'])\n",
        "plt.ylabel('Number of Rows')\n",
        "plt.title('Comparison of Number of Rows above and below the Median')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "lyNpdk6V6WWf"
      },
      "source": [
        "Further inspection using visuals showed that the sale price values above and below the median are significantly equal. This implies that\n",
        "The fact that the data is evenly distributed around the median indicates that the median is a representative value that effectively divides the dataset into two equal halves. We can safely conclude that the average price of goods sold is"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcuamBeD6WWf"
      },
      "source": [
        "### Most Frequently bought products' name\n",
        "To know the name of the product most frequently ordered, a WordCloud is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZl2noHf6WWf"
      },
      "outputs": [],
      "source": [
        "#Displaying the word cloud\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400,\n",
        "                    background_color='white').generate_from_frequencies(transformed_product_j['name_title'].value_counts())\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of the unique products')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "XbT-Wjzt6WWf"
      },
      "source": [
        "\n",
        "From the output of the code, it can be seen that the boldest, which implies the most frequently ordered or sold product, is the \"Champion Vapour Shorts,\" with \"Stafford Gunner Mens Cap Toe Leather Boots,\" \"Champion Vapour Short-Sleeve Tee,\" etc. clearly behind."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV0yFTFA6WWf"
      },
      "source": [
        "### Most favoured brands\n",
        "\n",
        "To have an idea of the most favoured brand, Word Cloud is used again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5vCoUtg6WWf"
      },
      "outputs": [],
      "source": [
        "#Displaying the Word Cloud\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400,\n",
        "                    background_color='white').generate_from_frequencies(transformed_product_j['brand'].value_counts())\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of the unique products')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "fP7Mz4zj6WWg"
      },
      "source": [
        "The ouput shows that the popular brands are ARIZONA, LIZ CLAIBORNE, XERSION, NIKE, LEVI, ST JOHN'S BAY e.t.c."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8riwrErR6WWg"
      },
      "source": [
        "### Most frequently used key words in the reviews\n",
        "\n",
        "In visualizing the review description, Word Cloud is used again, but with a slight modification.Stop words like the common articles and auxillary verbs in the English language (and, is, an, a, etc., excluding \"do\" and \"not\" because without these keywords, \"do not like\" will be classified as \"like\") had to be removed to have less diverse or unique keywords. The first attempt without removing keywords didn't give the desired result.\n",
        "\n",
        "The import list in cell 1 of the Jupiter Notebook was updated, necessary downloads were made, and libraries were imported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVra6XHU6WWg"
      },
      "outputs": [],
      "source": [
        "# Function to remove stop words\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))- {'do', 'not'}\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_text)\n",
        "\n",
        "# Applying the function to the 'Review' column\n",
        "transformed_review_c['Review_no_stopwords'] = transformed_review_c['Review'].apply(remove_stopwords)\n",
        "\n",
        "# Combining the new column created above into a single string\n",
        "text_data = ' '.join(transformed_review_c['Review_no_stopwords'])\n",
        "\n",
        "# Creating a WordCloud object with stopwords\n",
        "wordcloud = WordCloud(stopwords =stopwords.words('english') ,\n",
        "                      background_color='white', width=800, height=400).generate(text_data)\n",
        "\n",
        "# Displaying the WordCloud image\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "_OE00a6-6WWg"
      },
      "source": [
        "\"Love\" was the most frequently used word. Other words that denotes positive sentiment \"really like\", \"great quality\", \"fit perfect\" e.t.c can be seem in the cloud. Word denoting negative sentiments cannot be (easily) seen. This implies that many of the reviews were positive.\n",
        "\n",
        "The word \"returned\" needs to be assessed further, this could imply return of products by unsatisfied customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFge2k4h6WWg"
      },
      "source": [
        "### Sentiment Analysis of the Review Data\n",
        "\n",
        "The qunatitative analysis of the positive and negative reviews can be done using the sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DURl4IjX6WWg"
      },
      "outputs": [],
      "source": [
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Creating a function for sentiment analysis\n",
        "def analyze_sentiment(text):\n",
        "    sentiment_score = sid.polarity_scores(text)\n",
        "    return sentiment_score['compound']\n",
        "\n",
        "# Apply sentiment analysis to the 'Review' column and store the sentiment scores in a new column\n",
        "transformed_review_c['sentiment_score'] = transformed_review_c['Review'].apply(analyze_sentiment)\n",
        "\n",
        "# Classify sentiment based on the sentiment score\n",
        "transformed_review_c['sentiment'] = transformed_review_c['sentiment_score'].apply(lambda score:\n",
        "                                                    'Positive' if score > 0 else 'Negative' if score < 0 else 'Neutral')\n",
        "transformed_review_c\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYX2NfVU6WWg"
      },
      "source": [
        "The percentage of the bad reviews as well as the positive reviews can be calculated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkF_ESya6WWh"
      },
      "outputs": [],
      "source": [
        "# Filter out rows with negative sentiment scores\n",
        "negative_sentiment_rows = transformed_review_c[transformed_review_c['sentiment_score'] < 0]\n",
        "\n",
        "Percentage_negative_review = len(negative_sentiment_rows)/len(transformed_review_c)*100\n",
        "\n",
        "Percentage_Positive_review = 100 - Percentage_negative_review\n",
        "\n",
        "print(f\" The percentage positive review is {Percentage_Positive_review: .1f}% \" )\n",
        "print(f\" The percentage negative review is {Percentage_negative_review: .1f}% \" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8x6olF16WWh"
      },
      "outputs": [],
      "source": [
        "negative_sentiment_rows.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkOIBGRV6WWh"
      },
      "source": [
        "WordCloud can be used to check for most frequent keywords in the negative review description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUiUbbt_6WWh"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))- {'do', 'not'}\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_text)\n",
        "\n",
        "# Applying the function to the 'Review' column\n",
        "negative_sentiment_rows['Review_no_stopwords'] = negative_sentiment_rows['Review'].apply(remove_stopwords)\n",
        "\n",
        "# Combining the new column created above into a single string\n",
        "text_data = ' '.join(negative_sentiment_rows['Review_no_stopwords'])\n",
        "\n",
        "# Creating a WordCloud object with stopwords\n",
        "wordcloud = WordCloud(stopwords =stopwords.words('english') ,\n",
        "                      background_color='white', width=800, height=400).generate(text_data)\n",
        "\n",
        "# Displaying the WordCloud image\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "sHa_rcMy6WWh"
      },
      "source": [
        "The WordCloud didn't really provide much useful information on the most frequent issues that prompted the negative reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDBwN9ON6WWi"
      },
      "source": [
        "### Demographics (age bracket) of the users.\n",
        "\n",
        "To have a visual appreciation of the demographics, a simple yet effective form of visualization, the pie chart, is used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "py8Bhe3L6WWi"
      },
      "outputs": [],
      "source": [
        "#Plotiing a pie chart of\n",
        "fig = px.pie(user_review_j, names='Age_category', title='Age Distribution of Reviewers')\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "f0SdbKCA6WWi"
      },
      "source": [
        "The result showed that most reviewers are adult within the age bracket of 25 - 59 years."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YoUtL-K6WWi"
      },
      "source": [
        "### Distribution of Per Capita Income in the US\n",
        "\n",
        "Choropleth map will be best suited for this purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "F2xuuqms6WWi"
      },
      "outputs": [],
      "source": [
        "\n",
        "user_review_j['StateAbbreviation'] = user_review_j['State'].apply(lambda state:\n",
        "us.states.lookup(state).abbr if us.states.lookup(state) else state)\n",
        "\n",
        "\n",
        "fig = px.choropleth(user_review_j, locations='StateAbbreviation', color='State_frequency',\n",
        "                    locationmode='USA-states', scope='usa',\n",
        "                    title='Choropleth Map of State Frequency',\n",
        "                    color_continuous_scale='Viridis',\n",
        "                    range_color=[user_review_j['State_frequency'].min(), user_review_j['State_frequency'].max()],\n",
        "                    hover_data=['State', 'StateAbbreviation', 'State_frequency'])\n",
        "\n",
        "fig.update_geos(fitbounds=\"locations\", visible=False)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK3fj04C6WWj"
      },
      "source": [
        "### Distribution of Per Capita Income in the US\n",
        "\n",
        "Choropleth map will be best suited for this purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVeuY1G-6WWj"
      },
      "outputs": [],
      "source": [
        "\n",
        "edited_user_review['StateAbbreviation'] = edited_user_review['State'].apply(lambda state:\n",
        "us.states.lookup(state).abbr if us.states.lookup(state) else state)\n",
        "\n",
        "fig = px.choropleth(edited_user_review, locations='StateAbbreviation', color='Per_capita_income',\n",
        "                    locationmode='USA-states', scope='usa',\n",
        "                    title='Choropleth Map of Per Capita Income',\n",
        "                    color_continuous_scale='Viridis',\n",
        "                    range_color=[edited_user_review['Per_capita_income'].min(), edited_user_review['Per_capita_income'].max()],\n",
        "                    hover_data=['State', 'StateAbbreviation', 'Per_capita_income'])\n",
        "\n",
        "fig.update_geos(fitbounds=\"locations\", visible=False)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jl7iPSb6WWj"
      },
      "source": [
        "### Distribution of JC Penney Stores in the US\n",
        "\n",
        "Choropleth map will be best suited for this purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VXUeyZF6WWj"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(20, 6))\n",
        "ax = sn.barplot(x='State', y='Number of stores', data = edited_user_review, palette='viridis')\n",
        "plt.xlabel('States')\n",
        "plt.ylabel('Number of stores')\n",
        "plt.title('Stores in each State')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=8, color='black')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYElnF1j6WWj"
      },
      "source": [
        "# Assessing the correlation between the number of stores, income and reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXQew11N6WWj"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Scatter plot with regression line\n",
        "sn.regplot(x='Number of stores', y='Review per state', data=edited_user_review, scatter_kws={'s': 50}, line_kws={'color': 'red'})\n",
        "\n",
        "# Calculate the correlation coefficient\n",
        "corr_coef, _ = pearsonr(edited_user_review['Number of stores'], edited_user_review['Review per state'])\n",
        "\n",
        "# Add the correlation coefficient as text to the plot\n",
        "plt.text(0.7, 0.9, f'Correlation: {corr_coef:.2f}', transform=plt.gca().transAxes, fontsize=12)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMQaBrYl6WWk"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Scatter plot with regression line\n",
        "sn.regplot(x='Number of stores', y='Per_capita_income', data=edited_user_review, scatter_kws={'s': 50}, line_kws={'color': 'red'})\n",
        "\n",
        "# Calculate the correlation coefficient\n",
        "corr_coef, _ = pearsonr(edited_user_review['Number of stores'], edited_user_review['Per_capita_income'])\n",
        "\n",
        "# Add the correlation coefficient as text to the plot\n",
        "plt.text(0.7, 0.9, f'Correlation: {corr_coef:.2f}', transform=plt.gca().transAxes, fontsize=12)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1MK_kuR6WWk"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Scatter plot with regression line\n",
        "sn.regplot(x='Per_capita_income', y='Review per state', data=edited_user_review, scatter_kws={'s': 50}, line_kws={'color': 'red'})\n",
        "\n",
        "# Calculate the correlation coefficient\n",
        "corr_coef, _ = pearsonr(edited_user_review['Per_capita_income'], edited_user_review['Review per state'])\n",
        "\n",
        "# Add the correlation coefficient as text to the plot\n",
        "plt.text(0.7, 0.9, f'Correlation: {corr_coef:.2f}', transform=plt.gca().transAxes, fontsize=12)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "6BvcZc1E6WWk"
      },
      "source": [
        "The result shows that there might be correlation between income and number of stores.\n",
        "\n",
        "The other variables have no relationship whatsoever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vA7tEvm6WWk"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "edited_user_review['intercept'] = 1\n",
        "\n",
        "#Define the dependent and independent variables\n",
        "independent_vars = ['Per_capita_income', 'Number of stores', 'intercept']\n",
        "dependent_var = 'Review per state'\n",
        "\n",
        "# Perform multiple regression\n",
        "model = sm.OLS(edited_user_review[dependent_var], edited_user_review[independent_vars])\n",
        "results = model.fit()\n",
        "\n",
        "print(results.summary())"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "m7He6nn46WWl"
      },
      "source": [
        "To fix the multicollinearity problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hqmG07i6WWl"
      },
      "outputs": [],
      "source": [
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "\n",
        "# Check for multicollinearity\n",
        "X = edited_user_review[['Per_capita_income', 'Number of stores']]\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Variable\"] = X.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "print(\"VIF:\")\n",
        "print(vif)\n",
        "\n",
        "\n",
        "# Add a constant term for the intercept\n",
        "edited_user_review['intercept'] = 1\n",
        "\n",
        "# Perform multiple regression\n",
        "model = sm.OLS(edited_user_review['Review per state'], edited_user_review[['Per_capita_income', 'Number of stores', 'intercept']])\n",
        "results = model.fit()\n",
        "\n",
        "# Print regression summary\n",
        "print(results.summary())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "WWOJBR-P6WWl"
      },
      "source": [
        "        Couldn't fix the large condition number issue. Better analysis would be carried with more relevant data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4wCYIa26WWl"
      },
      "source": [
        "# SUMMARY OF FINDINGS\n",
        "\n",
        "1. The average sale price of goods, represented by the median of all the sale price is 35.47\n",
        "\n",
        "2. The most preffered brand by customers are : ARIZONA, LIZ CLAIBORNE, XERSION, NIKE, LEVI, ST JOHN'S BAY e.t.c.\n",
        "\n",
        "3. The most preffered products the customers are : \"Champion Vapour Shorts,\" with \"Stafford Gunner Mens Cap Toe Leather Boots,\" \"Champion Vapour Short-Sleeve Tee,\" etc.\n",
        "\n",
        "3. The age bracket that gave the most ratings and reviews on the product is 24â€“59 years old, followed by 60 and older, followed by young adults (16 - 23 years).\n",
        "\n",
        "4. Massachusetts, Vermont, and Delaware are the top three states where the review was done.\n",
        "\n",
        "5. The reviews by the customers were mostly positive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlAsG0iI6WWl"
      },
      "source": [
        "# CONCLUSION BASED ON FINDINGS\n",
        "\n",
        "1. The average income of the individualsÂ per monnth is 4826, and this value is way above the median sale price of the goods at JC Penney which is 35.47. This implies that the goods are quite affordable for the average income earner.\n",
        "\n",
        "2. Customers prefer some brands over others and some products over the others. Some of the reasons are listed in the review file. Apart from the fact that most requested products are relatively cheaper, many reviews described the products as satisfactory.\n",
        "\n",
        "3. Most reviewers, and by extension, shoppers, are adults within the age bracket of 24-59. This implies that the most demanded products will be ones tailored to the needs of adults. Products that generally appeal to older citizens and young adults will be demanded to a lesser degree.\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DYICPhq6WWm"
      },
      "source": [
        "# RECOMMENDATIONS\n",
        "The company's current approach appears to be producing decent outcomes. I would advise them to broaden on it and adapt to the times in order to understand the ever-changingÂ preferences of their clientele.\n",
        "\n",
        "The negative_sentiment_row data frame should be studied more closely.\n",
        "\n",
        "I have some recommendations for how to make data collection better. They are as follows:\n",
        "\n",
        "1. A lot of studies have been carried out on the effect of gender, age, and income on shopping behavior. The customer data given only captured the date of birth of customers (which was used to calculate their ages at the point when the data was gathered) who reviewed the products. This only gives us insight relating to the age distribution of the customers. There is a possibility that more insight can be gained if their average income is known. This data can be collected anonymously, and assurance should be given that the confidentiality agreement will not be breached.\n",
        "\n",
        "2. According to recent statistics, \"99.75% of online shoppers read reviews at least sometimes; 91% do so always or regularly. 98% of shoppers say reviews are an essential resource when making purchase decisions. Nearly half (45%) of consumers won't purchase a product if there are no reviews available for it.\" (Survey: The Ever-Growing Power of Reviews, May 11, 2023).\n",
        "Bosting reviews will definitely boost sales. The possible means to any factor that boost review should be considered. Number of stores and per capita income have little or no relationship with number of reviews. There might be more factors that work together to influence reviews, With more data such relationships can be explored adequately.\n",
        "\n",
        "4. The scores in the user review file don't seem to be consistent with the reviews. For instance,many positive reviews correspond with scores of 0 and 1. The word cloud also notes that positive comments like \"I love\" and \"very comfortable\" were more frequent than other words. Assigning the keywords excellent and very good to scores of 0 and 1, respectively, showed that there were many 0 and 1 scores. This could imply that many users either do not leave a rating score or assume that 1 (probably 0) implies a high rating. This can be fixed by designing the ratings to be more categorical than numerical."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}